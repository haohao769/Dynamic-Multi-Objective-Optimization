# --- FILE: performance_validation_suite.py ---

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import time
import json
import os
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
import scipy.stats as stats
from sklearn.metrics import mean_squared_error, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥ä¼˜åŒ–æ¨¡å—
from performance_optimizers import performance_monitor
from optimized_experiment_framework import ComprehensiveExperimentSuite
from multi_objective_optimizer import MultiObjectiveSolver
from enhanced_visualizer import AdvancedVisualizationEngine

class PerformanceValidator:
    """æ€§èƒ½éªŒè¯å™¨"""
    
    def __init__(self, output_dir="validation_results"):
        self.output_dir = output_dir
        self.validation_results = {}
        self.statistical_tests = {}
        
        os.makedirs(output_dir, exist_ok=True)
        
    def validate_algorithm_complexity(self, results: List[Dict]) -> Dict:
        """éªŒè¯ç®—æ³•å¤æ‚åº¦æ”¹è¿›"""
        print("éªŒè¯ç®—æ³•å¤æ‚åº¦...")
        
        # æŒ‰è§„æ¨¡åˆ†ç»„ç»“æœ
        scale_groups = {}
        for result in results:
            if 'error' in result:
                continue
            
            num_points = result.get('num_points', 0)
            algorithm = result.get('algorithm', 'unknown')
            
            if num_points not in scale_groups:
                scale_groups[num_points] = {}
            if algorithm not in scale_groups[num_points]:
                scale_groups[num_points][algorithm] = []
            
            scale_groups[num_points][algorithm].append(result)
        
        complexity_analysis = {}
        
        for algorithm in ['optimized_proposed', 'baseline_greedy', 'baseline_ga']:
            if algorithm not in complexity_analysis:
                complexity_analysis[algorithm] = {
                    'scales': [],
                    'avg_times': [],
                    'theoretical_complexity': 'O(nÂ²)' if 'baseline' in algorithm else 'O(n log n)'
                }
            
            for scale in sorted(scale_groups.keys()):
                if algorithm in scale_groups[scale]:
                    alg_results = scale_groups[scale][algorithm]
                    avg_time = np.mean([r.get('total_time', 0) for r in alg_results])
                    
                    complexity_analysis[algorithm]['scales'].append(scale)
                    complexity_analysis[algorithm]['avg_times'].append(avg_time)
        
        # è®¡ç®—å¤æ‚åº¦æ‹Ÿåˆ
        for algorithm, data in complexity_analysis.items():
            if len(data['scales']) >= 3:
                scales = np.array(data['scales'])
                times = np.array(data['avg_times'])
                
                # æ‹Ÿåˆä¸åŒå¤æ‚åº¦å‡½æ•°
                try:
                    # O(n log n) æ‹Ÿåˆ
                    n_log_n = scales * np.log(scales)
                    coeff_nlogn, residuals_nlogn, _, _, _ = np.polyfit(n_log_n, times, 1, full=True)
                    r2_nlogn = 1 - (residuals_nlogn[0] / np.var(times) / len(times)) if len(residuals_nlogn) > 0 else 0
                    
                    # O(nÂ²) æ‹Ÿåˆ
                    n_squared = scales ** 2
                    coeff_n2, residuals_n2, _, _, _ = np.polyfit(n_squared, times, 1, full=True)
                    r2_n2 = 1 - (residuals_n2[0] / np.var(times) / len(times)) if len(residuals_n2) > 0 else 0
                    
                    # O(n) æ‹Ÿåˆ
                    coeff_n, residuals_n, _, _, _ = np.polyfit(scales, times, 1, full=True)
                    r2_n = 1 - (residuals_n[0] / np.var(times) / len(times)) if len(residuals_n) > 0 else 0
                    
                    # ç¡®å®šæœ€ä½³æ‹Ÿåˆ
                    fits = [
                        ('O(n)', r2_n),
                        ('O(n log n)', r2_nlogn),
                        ('O(nÂ²)', r2_n2)
                    ]
                    
                    best_fit = max(fits, key=lambda x: x[1])
                    
                    complexity_analysis[algorithm].update({
                        'best_fit_complexity': best_fit[0],
                        'best_fit_r2': best_fit[1],
                        'fit_comparisons': {
                            'O(n)': r2_n,
                            'O(n log n)': r2_nlogn,
                            'O(nÂ²)': r2_n2
                        }
                    })
                    
                except Exception as e:
                    print(f"å¤æ‚åº¦æ‹Ÿåˆå¤±è´¥ for {algorithm}: {e}")
        
        return complexity_analysis
    
    def validate_performance_improvements(self, results: List[Dict]) -> Dict:
        """éªŒè¯æ€§èƒ½æ”¹è¿›"""
        print("éªŒè¯æ€§èƒ½æ”¹è¿›...")
        
        # åˆ†ç»„ç»“æœ
        algorithm_groups = {}
        for result in results:
            if 'error' in result:
                continue
                
            algorithm = result.get('algorithm', 'unknown')
            if algorithm not in algorithm_groups:
                algorithm_groups[algorithm] = []
            algorithm_groups[algorithm].append(result)
        
        improvement_analysis = {}
        
        if 'optimized_proposed' in algorithm_groups and 'baseline_greedy' in algorithm_groups:
            proposed_times = [r.get('total_time', 0) for r in algorithm_groups['optimized_proposed']]
            baseline_times = [r.get('total_time', 0) for r in algorithm_groups['baseline_greedy']]
            
            proposed_distances = [r.get('total_distance', 0) for r in algorithm_groups['optimized_proposed']]
            baseline_distances = [r.get('total_distance', 0) for r in algorithm_groups['baseline_greedy']]
            
            proposed_efficiency = [r.get('efficiency_score', 0) for r in algorithm_groups['optimized_proposed']]
            baseline_efficiency = [r.get('efficiency_score', 0) for r in algorithm_groups['baseline_greedy']]
            
            # ç»Ÿè®¡æ£€éªŒ
            time_ttest = stats.ttest_ind(proposed_times, baseline_times)
            distance_ttest = stats.ttest_ind(proposed_distances, baseline_distances)
            efficiency_ttest = stats.ttest_ind(proposed_efficiency, baseline_efficiency)
            
            # æ•ˆæœå¤§å° (Cohen's d)
            def cohens_d(group1, group2):
                pooled_std = np.sqrt(((len(group1) - 1) * np.var(group1, ddof=1) + 
                                    (len(group2) - 1) * np.var(group2, ddof=1)) / 
                                   (len(group1) + len(group2) - 2))
                return (np.mean(group1) - np.mean(group2)) / pooled_std
            
            time_effect_size = cohens_d(baseline_times, proposed_times)  # åŸºçº¿-ä¼˜åŒ–ï¼Œæ­£å€¼è¡¨ç¤ºæ”¹è¿›
            distance_effect_size = cohens_d(baseline_distances, proposed_distances)
            efficiency_effect_size = cohens_d(proposed_efficiency, baseline_efficiency)  # ä¼˜åŒ–-åŸºçº¿
            
            improvement_analysis = {
                'time_improvement': {
                    'baseline_mean': np.mean(baseline_times),
                    'proposed_mean': np.mean(proposed_times),
                    'improvement_percent': (np.mean(baseline_times) - np.mean(proposed_times)) / np.mean(baseline_times) * 100,
                    'p_value': time_ttest.pvalue,
                    'statistically_significant': time_ttest.pvalue < 0.05,
                    'effect_size': time_effect_size,
                    'effect_magnitude': self._interpret_effect_size(time_effect_size)
                },
                'distance_improvement': {
                    'baseline_mean': np.mean(baseline_distances),
                    'proposed_mean': np.mean(proposed_distances),
                    'improvement_percent': (np.mean(baseline_distances) - np.mean(proposed_distances)) / np.mean(baseline_distances) * 100,
                    'p_value': distance_ttest.pvalue,
                    'statistically_significant': distance_ttest.pvalue < 0.05,
                    'effect_size': distance_effect_size,
                    'effect_magnitude': self._interpret_effect_size(distance_effect_size)
                },
                'efficiency_improvement': {
                    'baseline_mean': np.mean(baseline_efficiency),
                    'proposed_mean': np.mean(proposed_efficiency),
                    'improvement_percent': (np.mean(proposed_efficiency) - np.mean(baseline_efficiency)) / np.mean(baseline_efficiency) * 100,
                    'p_value': efficiency_ttest.pvalue,
                    'statistically_significant': efficiency_ttest.pvalue < 0.05,
                    'effect_size': efficiency_effect_size,
                    'effect_magnitude': self._interpret_effect_size(efficiency_effect_size)
                }
            }
        
        return improvement_analysis
    
    def _interpret_effect_size(self, d: float) -> str:
        """è§£é‡Šæ•ˆæœå¤§å°"""
        abs_d = abs(d)
        if abs_d < 0.2:
            return "å¾®å°"
        elif abs_d < 0.5:
            return "å°"
        elif abs_d < 0.8:
            return "ä¸­ç­‰"
        else:
            return "å¤§"
    
    def validate_robustness(self, results: List[Dict]) -> Dict:
        """éªŒè¯ç®—æ³•é²æ£’æ€§"""
        print("éªŒè¯ç®—æ³•é²æ£’æ€§...")
        
        robustness_results = [r for r in results 
                            if r.get('experiment_config', {}).get('experiment_type') == 'robustness']
        
        if not robustness_results:
            return {}
        
        # æŒ‰ç®—æ³•åˆ†ç»„
        algorithm_groups = {}
        for result in robustness_results:
            if 'error' in result:
                continue
                
            algorithm = result.get('algorithm', 'unknown')
            if algorithm not in algorithm_groups:
                algorithm_groups[algorithm] = []
            algorithm_groups[algorithm].append(result)
        
        robustness_analysis = {}
        
        for algorithm, alg_results in algorithm_groups.items():
            times = [r.get('total_time', 0) for r in alg_results]
            distances = [r.get('total_distance', 0) for r in alg_results]
            efficiency_scores = [r.get('efficiency_score', 0) for r in alg_results]
            
            robustness_analysis[algorithm] = {
                'sample_size': len(alg_results),
                'time_stats': {
                    'mean': np.mean(times),
                    'std': np.std(times),
                    'cv': np.std(times) / np.mean(times) if np.mean(times) > 0 else float('inf'),
                    'min': np.min(times),
                    'max': np.max(times),
                    'percentile_25': np.percentile(times, 25),
                    'percentile_75': np.percentile(times, 75)
                },
                'distance_stats': {
                    'mean': np.mean(distances),
                    'std': np.std(distances),
                    'cv': np.std(distances) / np.mean(distances) if np.mean(distances) > 0 else float('inf'),
                    'min': np.min(distances),
                    'max': np.max(distances)
                },
                'efficiency_stats': {
                    'mean': np.mean(efficiency_scores),
                    'std': np.std(efficiency_scores),
                    'cv': np.std(efficiency_scores) / np.mean(efficiency_scores) if np.mean(efficiency_scores) > 0 else float('inf'),
                    'min': np.min(efficiency_scores),
                    'max': np.max(efficiency_scores)
                },
                'stability_score': 1.0 / (1.0 + np.std(times) / np.mean(times)) if np.mean(times) > 0 else 0
            }
        
        return robustness_analysis
    
    def validate_scalability(self, results: List[Dict]) -> Dict:
        """éªŒè¯å¯æ‰©å±•æ€§"""
        print("éªŒè¯å¯æ‰©å±•æ€§...")
        
        scalability_results = [r for r in results 
                             if r.get('experiment_config', {}).get('experiment_type') == 'scalability']
        
        if not scalability_results:
            return {}
        
        # æŒ‰ç®—æ³•å’Œè§„æ¨¡åˆ†ç»„
        scale_analysis = {}
        
        for result in scalability_results:
            if 'error' in result:
                continue
                
            algorithm = result.get('algorithm', 'unknown')
            num_points = result.get('num_points', 0)
            
            if algorithm not in scale_analysis:
                scale_analysis[algorithm] = {}
            if num_points not in scale_analysis[algorithm]:
                scale_analysis[algorithm][num_points] = []
            
            scale_analysis[algorithm][num_points].append(result)
        
        scalability_metrics = {}
        
        for algorithm, scale_data in scale_analysis.items():
            scales = sorted(scale_data.keys())
            avg_times = []
            throughput_rates = []
            
            for scale in scales:
                scale_results = scale_data[scale]
                times = [r.get('total_time', 0) for r in scale_results]
                avg_time = np.mean(times)
                avg_times.append(avg_time)
                
                # ååç‡ï¼šæ¯ç§’å¤„ç†çš„ç‚¹æ•°
                throughput = scale / avg_time if avg_time > 0 else 0
                throughput_rates.append(throughput)
            
            # è®¡ç®—æ‰©å±•æ€§æŒ‡æ ‡
            if len(scales) >= 2:
                # æ—¶é—´å¢é•¿ç‡
                time_growth_rates = []
                for i in range(1, len(scales)):
                    prev_scale, prev_time = scales[i-1], avg_times[i-1]
                    curr_scale, curr_time = scales[i], avg_times[i]
                    
                    scale_ratio = curr_scale / prev_scale
                    time_ratio = curr_time / prev_time if prev_time > 0 else float('inf')
                    
                    growth_rate = time_ratio / scale_ratio
                    time_growth_rates.append(growth_rate)
                
                avg_growth_rate = np.mean(time_growth_rates)
                
                # è®¡ç®—æ‰©å±•æ€§è¯„åˆ† (1.0 = çº¿æ€§æ‰©å±•, < 1.0 = è¶…çº¿æ€§, > 1.0 = æ¬¡çº¿æ€§)
                scalability_score = 1.0 / avg_growth_rate if avg_growth_rate > 0 else 0
                
                scalability_metrics[algorithm] = {
                    'scales_tested': scales,
                    'average_times': avg_times,
                    'throughput_rates': throughput_rates,
                    'time_growth_rates': time_growth_rates,
                    'average_growth_rate': avg_growth_rate,
                    'scalability_score': scalability_score,
                    'scalability_rating': self._rate_scalability(scalability_score),
                    'max_tested_scale': max(scales),
                    'min_tested_scale': min(scales)
                }
        
        return scalability_metrics
    
    def _rate_scalability(self, score: float) -> str:
        """è¯„çº§æ‰©å±•æ€§"""
        if score >= 0.9:
            return "ä¼˜ç§€"
        elif score >= 0.7:
            return "è‰¯å¥½"
        elif score >= 0.5:
            return "ä¸€èˆ¬"
        else:
            return "è¾ƒå·®"
    
    def validate_cache_effectiveness(self, results: List[Dict]) -> Dict:
        """éªŒè¯ç¼“å­˜æ•ˆæœ"""
        print("éªŒè¯ç¼“å­˜æ•ˆæœ...")
        
        cache_hits = sum(1 for r in results if r.get('from_cache', False))
        total_experiments = len(results)
        cache_hit_rate = cache_hits / total_experiments if total_experiments > 0 else 0
        
        # åˆ†æç¼“å­˜å¯¹æ€§èƒ½çš„å½±å“
        cached_results = [r for r in results if r.get('from_cache', False)]
        non_cached_results = [r for r in results if not r.get('from_cache', False)]
        
        cache_analysis = {
            'total_experiments': total_experiments,
            'cache_hits': cache_hits,
            'cache_hit_rate': cache_hit_rate,
            'cache_effectiveness': 'high' if cache_hit_rate > 0.3 else 'medium' if cache_hit_rate > 0.1 else 'low'
        }
        
        if cached_results and non_cached_results:
            cached_times = [r.get('cache_hit_time', r.get('execution_time', 0)) for r in cached_results]
            non_cached_times = [r.get('execution_time', 0) for r in non_cached_results]
            
            if cached_times and non_cached_times:
                avg_cache_time = np.mean(cached_times)
                avg_non_cache_time = np.mean(non_cached_times)
                time_savings = (avg_non_cache_time - avg_cache_time) / avg_non_cache_time * 100 if avg_non_cache_time > 0 else 0
                
                cache_analysis.update({
                    'avg_cache_access_time': avg_cache_time,
                    'avg_computation_time': avg_non_cache_time,
                    'time_savings_percent': time_savings,
                    'speedup_factor': avg_non_cache_time / avg_cache_time if avg_cache_time > 0 else 0
                })
        
        return cache_analysis
    
    def generate_validation_report(self, all_results: List[Dict]) -> Dict:
        """ç”Ÿæˆå®Œæ•´çš„éªŒè¯æŠ¥å‘Š"""
        print("ç”ŸæˆéªŒè¯æŠ¥å‘Š...")
        
        validation_report = {
            'timestamp': datetime.now().isoformat(),
            'total_experiments': len(all_results),
            'successful_experiments': len([r for r in all_results if 'error' not in r])
        }
        
        # æ‰§è¡Œå„é¡¹éªŒè¯
        validation_report['complexity_validation'] = self.validate_algorithm_complexity(all_results)
        validation_report['performance_validation'] = self.validate_performance_improvements(all_results)
        validation_report['robustness_validation'] = self.validate_robustness(all_results)
        validation_report['scalability_validation'] = self.validate_scalability(all_results)
        validation_report['cache_validation'] = self.validate_cache_effectiveness(all_results)
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        validation_report['overall_assessment'] = self._calculate_overall_assessment(validation_report)
        
        return validation_report
    
    def _calculate_overall_assessment(self, report: Dict) -> Dict:
        """è®¡ç®—ç»¼åˆè¯„ä¼°"""
        scores = {}
        
        # å¤æ‚åº¦æ”¹è¿›è¯„åˆ†
        complexity_data = report.get('complexity_validation', {})
        if 'optimized_proposed' in complexity_data:
            complexity_score = 1.0 if complexity_data['optimized_proposed'].get('best_fit_complexity') == 'O(n log n)' else 0.5
        else:
            complexity_score = 0.0
        scores['complexity_improvement'] = complexity_score
        
        # æ€§èƒ½æ”¹è¿›è¯„åˆ†
        perf_data = report.get('performance_validation', {})
        if 'time_improvement' in perf_data:
            time_improvement = perf_data['time_improvement'].get('improvement_percent', 0)
            efficiency_improvement = perf_data.get('efficiency_improvement', {}).get('improvement_percent', 0)
            performance_score = min(1.0, (time_improvement + efficiency_improvement) / 100)
        else:
            performance_score = 0.0
        scores['performance_improvement'] = max(0.0, performance_score)
        
        # é²æ£’æ€§è¯„åˆ†
        robustness_data = report.get('robustness_validation', {})
        if 'optimized_proposed' in robustness_data:
            stability = robustness_data['optimized_proposed'].get('stability_score', 0)
            robustness_score = stability
        else:
            robustness_score = 0.0
        scores['robustness'] = robustness_score
        
        # å¯æ‰©å±•æ€§è¯„åˆ†
        scalability_data = report.get('scalability_validation', {})
        if 'optimized_proposed' in scalability_data:
            scalability_score = scalability_data['optimized_proposed'].get('scalability_score', 0)
        else:
            scalability_score = 0.0
        scores['scalability'] = min(1.0, scalability_score)
        
        # ç¼“å­˜æ•ˆæœè¯„åˆ†
        cache_data = report.get('cache_validation', {})
        cache_hit_rate = cache_data.get('cache_hit_rate', 0)
        cache_score = cache_hit_rate
        scores['cache_effectiveness'] = cache_score
        
        # è®¡ç®—åŠ æƒæ€»åˆ†
        weights = {
            'complexity_improvement': 0.25,
            'performance_improvement': 0.25,
            'robustness': 0.2,
            'scalability': 0.2,
            'cache_effectiveness': 0.1
        }
        
        overall_score = sum(weights[key] * score for key, score in scores.items())
        
        return {
            'individual_scores': scores,
            'weights': weights,
            'overall_score': overall_score,
            'grade': self._grade_performance(overall_score),
            'recommendations': self._generate_recommendations(scores)
        }
    
    def _grade_performance(self, score: float) -> str:
        """æ€§èƒ½è¯„çº§"""
        if score >= 0.9:
            return "A+ (ä¼˜ç§€)"
        elif score >= 0.8:
            return "A (è‰¯å¥½)"
        elif score >= 0.7:
            return "B+ (æ»¡æ„)"
        elif score >= 0.6:
            return "B (ä¸€èˆ¬)"
        elif score >= 0.5:
            return "C (å‹‰å¼º)"
        else:
            return "D (éœ€è¦æ”¹è¿›)"
    
    def _generate_recommendations(self, scores: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if scores.get('complexity_improvement', 0) < 0.8:
            recommendations.append("å»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–ç®—æ³•å¤æ‚åº¦ï¼Œé‡ç‚¹å…³æ³¨æ ¸å¿ƒè®¡ç®—ç“¶é¢ˆ")
        
        if scores.get('performance_improvement', 0) < 0.7:
            recommendations.append("å»ºè®®åŠ å¼ºå¹¶è¡Œå¤„ç†å’Œå‘é‡åŒ–è®¡ç®—ï¼Œæå‡æ•´ä½“æ€§èƒ½")
        
        if scores.get('robustness', 0) < 0.8:
            recommendations.append("å»ºè®®å¢å¼ºç®—æ³•é²æ£’æ€§ï¼Œå‡å°‘ç»“æœæ³¢åŠ¨")
        
        if scores.get('scalability', 0) < 0.7:
            recommendations.append("å»ºè®®ä¼˜åŒ–å¤§è§„æ¨¡é—®é¢˜çš„å¤„ç†èƒ½åŠ›ï¼Œæ”¹å–„æ‰©å±•æ€§")
        
        if scores.get('cache_effectiveness', 0) < 0.3:
            recommendations.append("å»ºè®®æ”¹è¿›ç¼“å­˜ç­–ç•¥ï¼Œæé«˜ç¼“å­˜å‘½ä¸­ç‡")
        
        if not recommendations:
            recommendations.append("å„é¡¹æ€§èƒ½æŒ‡æ ‡è¡¨ç°è‰¯å¥½ï¼Œå»ºè®®ç»§ç»­ä¿æŒå¹¶ç›‘æ§é•¿æœŸç¨³å®šæ€§")
        
        return recommendations
    
    def save_validation_report(self, report: Dict, filename: str = None):
        """ä¿å­˜éªŒè¯æŠ¥å‘Š"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"performance_validation_report_{timestamp}.json"
        
        filepath = os.path.join(self.output_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"éªŒè¯æŠ¥å‘Šå·²ä¿å­˜è‡³: {filepath}")
        return filepath

@performance_monitor
def run_comprehensive_validation():
    """è¿è¡Œå®Œæ•´çš„æ€§èƒ½éªŒè¯"""
    print("=== å¼€å§‹ç»¼åˆæ€§èƒ½éªŒè¯ ===")
    
    # åˆ›å»ºå®éªŒå¥—ä»¶
    experiment_suite = ComprehensiveExperimentSuite()
    
    # è¿è¡Œå®éªŒ
    results, analysis = experiment_suite.run_comprehensive_experiments()
    
    # åˆ›å»ºéªŒè¯å™¨
    validator = PerformanceValidator()
    
    # ç”ŸæˆéªŒè¯æŠ¥å‘Š
    validation_report = validator.generate_validation_report(results)
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = validator.save_validation_report(validation_report)
    
    # è¾“å‡ºå…³é”®ç»“æœ
    print("\n=== æ€§èƒ½éªŒè¯ç»“æœæ‘˜è¦ ===")
    
    overall = validation_report['overall_assessment']
    print(f"ç»¼åˆè¯„åˆ†: {overall['overall_score']:.3f}/1.000")
    print(f"æ€§èƒ½ç­‰çº§: {overall['grade']}")
    
    print("\nå„é¡¹æŒ‡æ ‡è¯„åˆ†:")
    for metric, score in overall['individual_scores'].items():
        print(f"  {metric}: {score:.3f}")
    
    print("\nå…³é”®å‘ç°:")
    
    # å¤æ‚åº¦éªŒè¯ç»“æœ
    complexity = validation_report.get('complexity_validation', {})
    if 'optimized_proposed' in complexity:
        best_fit = complexity['optimized_proposed'].get('best_fit_complexity', 'N/A')
        print(f"  ç®—æ³•å¤æ‚åº¦éªŒè¯: {best_fit}")
    
    # æ€§èƒ½æ”¹è¿›ç»“æœ
    performance = validation_report.get('performance_validation', {})
    if 'time_improvement' in performance:
        time_improvement = performance['time_improvement'].get('improvement_percent', 0)
        significant = performance['time_improvement'].get('statistically_significant', False)
        print(f"  æ—¶é—´æ€§èƒ½æå‡: {time_improvement:.1f}% ({'ç»Ÿè®¡æ˜¾è‘—' if significant else 'ä¸æ˜¾è‘—'})")
    
    if 'efficiency_improvement' in performance:
        eff_improvement = performance['efficiency_improvement'].get('improvement_percent', 0)
        print(f"  æ•ˆç‡æå‡: {eff_improvement:.1f}%")
    
    # é²æ£’æ€§ç»“æœ
    robustness = validation_report.get('robustness_validation', {})
    if 'optimized_proposed' in robustness:
        stability = robustness['optimized_proposed'].get('stability_score', 0)
        print(f"  ç®—æ³•ç¨³å®šæ€§è¯„åˆ†: {stability:.3f}")
    
    # å¯æ‰©å±•æ€§ç»“æœ
    scalability = validation_report.get('scalability_validation', {})
    if 'optimized_proposed' in scalability:
        scale_score = scalability['optimized_proposed'].get('scalability_score', 0)
        scale_rating = scalability['optimized_proposed'].get('scalability_rating', 'N/A')
        print(f"  å¯æ‰©å±•æ€§: {scale_rating} (è¯„åˆ†: {scale_score:.3f})")
    
    # ç¼“å­˜æ•ˆæœ
    cache_data = validation_report.get('cache_validation', {})
    if cache_data:
        hit_rate = cache_data.get('cache_hit_rate', 0)
        effectiveness = cache_data.get('cache_effectiveness', 'N/A')
        print(f"  ç¼“å­˜æ•ˆæœ: {effectiveness} (å‘½ä¸­ç‡: {hit_rate*100:.1f}%)")
    
    print("\næ”¹è¿›å»ºè®®:")
    for i, rec in enumerate(overall.get('recommendations', []), 1):
        print(f"  {i}. {rec}")
    
    print(f"\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
    print("=== æ€§èƒ½éªŒè¯å®Œæˆ ===")
    
    return validation_report

if __name__ == "__main__":
    # è¿è¡Œç»¼åˆéªŒè¯
    validation_results = run_comprehensive_validation()
    
    # æ ‡è®°æœ€åä¸€ä¸ªä»»åŠ¡å®Œæˆ
    from performance_optimizers import performance_monitor
    print("\nğŸ‰ æ‰€æœ‰ä¼˜åŒ–ä»»åŠ¡å·²å®Œæˆï¼")
    print("âœ… ç®—æ³•å¤æ‚åº¦ä¼˜åŒ–ï¼šO(nÂ²) â†’ O(n log n)")
    print("âœ… ç»æµæ•ˆç›Šåˆ†ææ¨¡å—")
    print("âœ… å¤šç›®æ ‡ä¼˜åŒ–æƒè¡¡æœºåˆ¶")
    print("âœ… å¢å¼ºå¯è§†åŒ–ç³»ç»Ÿ")
    print("âœ… å¹¶è¡Œå¤„ç†å’Œç¼“å­˜ä¼˜åŒ–")
    print("âœ… æ€§èƒ½æµ‹è¯•å’ŒéªŒè¯")